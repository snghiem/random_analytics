{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/Applications/Python-3.5.2/Build/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "df = pd.read_csv(\"Iris_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['Labels']\n",
    "x = df.iloc[:,:4]\n",
    "\n",
    "# a) Split your data into train and test data.\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       0.86      1.00      0.92         6\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# b) Fit a decision tree classifier. How does it perform?\n",
    "ytree_pred = DecisionTreeClassifier().fit(xtrain, ytrain).predict(xtest)\n",
    "print(classification_report(ytest, ytree_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs well for this dataset with the average F1-score of .97 (I am genuinely interested in using F1-score to compare the effectiveness of a classifier as it is a harmonic mean between precision and recall. Normally I use cross validation or bootstrapping to decrease overfitting over one sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        13\n",
      "          2       1.00      1.00      1.00         6\n",
      "\n",
      "avg / total       1.00      1.00      1.00        30\n",
      "\n",
      "k= 10\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       0.86      1.00      0.92         6\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n",
      "k= 20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       0.86      1.00      0.92         6\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n",
      "k= 50\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       0.86      1.00      0.92         6\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n",
      "k= 80\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        11\n",
      "          1       1.00      0.08      0.14        13\n",
      "          2       0.35      1.00      0.52         6\n",
      "\n",
      "avg / total       0.84      0.60      0.52        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# c) What about a K-Nearest Neighbors model? Please try with 1, 10, 20, 50 and 80 neighbors. \n",
    "# Why do you think 50 and 80 neighbors works less well (answer in one or two sentences)?\n",
    "\n",
    "ks = [1,10,20,50,80]\n",
    "for k in ks:\n",
    "    yknn_pred = KNeighborsClassifier(n_neighbors =k).fit(xtrain,ytrain).predict(xtest)\n",
    "    print(\"k=\",k)\n",
    "    print(classification_report(ytest, yknn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cases of k=50 and 80 perform less well as the bigger k is, the less flexible a knn classifier becomes and the decision boundary becomes flatter to linear, reducing the accuracy of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       0.86      1.00      0.92         6\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yrf_pred = RandomForestClassifier().fit(xtrain, ytrain).predict(xtest)\n",
    "print(classification_report(ytest, yrf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, both decision tree and random forest perform effectively with equivalent f1-score. However this is only one sample dataset. Following I will proceed with a cross validation to reduce the overfitting of one dataset bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.044444444444444439"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree\n",
    "cross_val_score(DecisionTreeClassifier(), x,y, scoring = 'neg_mean_squared_error', cv=20).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.036111111111111108"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(RandomForestClassifier(), x,y, scoring = 'neg_mean_squared_error', cv=20).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier works a bit better as its absolute value of negative mean squared error is smaller but repeatedly with different cross validation, the results of both are not much different."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
